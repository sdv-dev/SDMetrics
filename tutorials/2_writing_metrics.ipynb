{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Custom Metrics\n",
    "Coming soon... how to write your own metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdmetrics import adversary\n",
    "from sdmetrics import MetricsReport\n",
    "\n",
    "report = MetricsReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Metric\n",
    "The simplest way to create a custom metric is to use the generic metric API. You simply write a function which yields a sequence of Metric objects, attach it to a metrics report, and you're ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdmetrics import Metric\n",
    "\n",
    "def my_custom_metrics(metadata, real_tables, synthetic_tables):\n",
    "    name = \"abs-diff-in-number-of-rows\"\n",
    "\n",
    "    for table_name in metadata:\n",
    "\n",
    "        # Absolute difference in number of rows\n",
    "        nb_real_rows = len(real_tables[table_name])\n",
    "        nb_synthetic_rows = len(synthetic_tables[table_name])\n",
    "        value = abs(nb_real_rows - nb_synthetic_rows)\n",
    "\n",
    "        # Specify some useful tags for the user\n",
    "        tags = set([\n",
    "            \"priority:high\",\n",
    "            \"table:%s\" % table_name\n",
    "        ])\n",
    "\n",
    "        yield Metric(name, value, tags)\n",
    "        \n",
    "report.add_metrics(my_custom_metrics(metadata, real_tables, fake_tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistic Metric\n",
    "Alternatively, if you're looking to create a statistical metric which looks at univariate or bivariate distributions, you can subclass the `UnivariateMetric` class and fill in a single function. The base class will handle identifying the columns which have the correct data type, traversing the tables, and so on. You can simply focus on the math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "from sdmetrics import Goal\n",
    "from sdmetrics.statistical import UnivariateMetric\n",
    "from sdmetrics.statistical.utils import frequencies\n",
    "\n",
    "class CSTest(UnivariateMetric):\n",
    "\n",
    "    name = \"chisquare\"\n",
    "    dtypes = [\"object\", \"bool\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def metric(real_column, fake_column):\n",
    "        \"\"\"This function uses the Chi-squared test to compare the distributions\n",
    "        of the two categorical columns. It returns the resulting p-value so that\n",
    "        a small value indicates that we can reject the null hypothesis (i.e. and\n",
    "        suggests that the distributions are different).\n",
    "\n",
    "        Arguments:\n",
    "            real_column (np.ndarray): The values from the real database.\n",
    "            fake_column (np.ndarray): The values from the fake database.\n",
    "\n",
    "        Returns:\n",
    "            (str, Goal, str, tuple): A tuple containing (value, goal, unit, domain)\n",
    "            which corresponds to the fields in a Metric object.\n",
    "        \"\"\"\n",
    "        f_obs, f_exp = frequencies(real_column, fake_column)\n",
    "        statistic, pvalue = chisquare(f_obs, f_exp)\n",
    "        return pvalue, Goal.MAXIMIZE, \"p-value\", (0.0, 1.0)\n",
    "\n",
    "report.add_metrics(CSTest.metrics(metadata, real_tables, fake_tables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection Metric\n",
    "Similarly, if you're looking to create a detection metric, you can subclass the `TabularDetector` class and fill in the `fit` and `predict_proba` functions. The base class will handle denormalizing parent-child relationships, etc. so you can focus on the machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sdmetrics.detection import TabularDetector\n",
    "\n",
    "\n",
    "class SVCDetector(TabularDetector):\n",
    "\n",
    "    name = \"svc\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"This function trains a sklearn pipeline with a robust scalar\n",
    "        and a support vector classifier.\n",
    "\n",
    "        Arguments:\n",
    "            X (np.ndarray): The numerical features (i.e. transformed rows).\n",
    "            y (np.ndarray): The binary classification target.\n",
    "        \"\"\"\n",
    "        self.model = Pipeline([\n",
    "            ('scalar', RobustScaler()),\n",
    "            ('classifier', SVC(probability=True, gamma='scale')),\n",
    "        ])\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)[:, 1]\n",
    "\n",
    "report.add_metrics(SVCDetector.metrics(metadata, real_tables, fake_tables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
